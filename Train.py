import osimport cudafrom cuda import DeviceDataLoaderimport Generative_Discriminative_modelsimport torchfrom torch.utils.data import DataLoaderfrom torchvision.datasets import ImageFolderfrom torchvision.utils import make_gridimport matplotlib.pyplot as pltimport torchvision.transforms as ttimport argparseimport osimport randomimport torch.nn as nnimport torch.nn.parallelimport torch.backends.cudnn as cudnnimport torch.optim as optimimport torch.utils.dataimport torchvision.datasets as dsetimport torchvision.transforms as transformsimport torchvision.utils as vutilsimport numpy as npimport matplotlib.animation as animationfrom IPython.display import HTMLimport torch.nn as nnimport torch.nn.functional as F# Set random seed for reproducibilitymanualSeed = 999#manualSeed = random.randint(1, 10000) # use if you want new resultsprint("Random Seed: ", manualSeed)random.seed(manualSeed)torch.manual_seed(manualSeed)def saveCheckpoint(Generative_XtoY_Model, Generative_YtoX_Model, Discriminative_X_Model, Discriminative_Y_Model, checkpoint_dir='/save/'):    """    Saves the parameters of both generators and discriminators.    """    #Path    Generative_XtoY_path = os.path.join(checkpoint_dir, 'Generative_XtoY_Model.pkl')    Generative_YtoX_path = os.path.join(checkpoint_dir, 'Generative_YtoX_Model.pkl')    Discriminative_X_path = os.path.join(checkpoint_dir, 'Discriminative_X_Model.pkl')    Discriminative_Y_path = os.path.join(checkpoint_dir, 'Discriminative_Y_Model.pkl')    #Saving    torch.save(Generative_XtoY_Model.state_dict(), Generative_XtoY_path)    torch.save(Generative_YtoX_Model.state_dict(), Generative_YtoX_path)    torch.save(Discriminative_X_Model.state_dict(), Discriminative_X_path)    torch.save(Discriminative_Y_Model.state_dict(), Discriminative_Y_path)def loadCheckpoint(directory):    """    Load checkoint    """    #model.load_state_dict(torch.load(checkpoint_path))    file = torch.load(directory, map_location=None)    print(' [*] Loading checkpoint from %s succeed!' % directory)    return filedef reNormalize(BatchOfImage, meanValue=0.5, stdValue=0.5):    if torch.is_tensor(BatchOfImage):        BatchOfImage = BatchOfImage.detach().numpy()    value=255    reNormalized_image = (BatchOfImage * stdValue * value) + (meanValue * value)    return np.uint8(reNormalized_image)def show_test(BatchOfY, BatchOfX, Generative_YtoX_Model, Generative_XtoY_Model, meanValue=0.5, stdValue=0.5):    """    Shows results of generates based on test image input.     """    #Identify correct device    #device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")    device=cuda.cuda_default()    #Create fake pictures for both cycles    XFake = Generative_YtoX_Model(BatchOfY.to(device))    YFake = Generative_XtoY_Model(BatchOfX.to(device))    #Generate grids    Xgrid =  make_grid(BatchOfX, nrow=4).permute(1, 2, 0).detach().cpu().numpy()    Ygrid =  make_grid(BatchOfY, nrow=4).permute(1, 2, 0).detach().cpu().numpy()    FakeXgrid =  make_grid(XFake, nrow=4).permute(1, 2, 0).detach().cpu().numpy()    FakeYgrid =  make_grid(YFake, nrow=4).permute(1, 2, 0).detach().cpu().numpy()    #Normalize pictures to pixel range rom 0 to 255    X, XFake = reNormalize(Xgrid, meanValue, stdValue), reNormalize(FakeXgrid, meanValue, stdValue)    Y, YFake = reNormalize(Ygrid, meanValue, stdValue), reNormalize(FakeYgrid, meanValue, stdValue)        #Transformation from X -> Y    figure, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True,squeeze=True, width_ratios=None, height_ratios=None, figsize=(20, 10))    ax1.imshow(X)    ax1.axis('off')    ax1.set_title('ORIGINAL_PICTURE')    ax2.imshow(YFake)    ax2.axis('off')    ax2.set_title('Fake Y  (Vangogh)')    plt.show()# Paths et ceteraSavePath = '/save/'DIRECTORY = "dataset/VanGoghStyle"DIRECTORY2 = "dataset/landscape"#dirs = os.listdir( data_dir )#print(os.getcwd())  Dataset = ImageFolder(DIRECTORY)print(len(Dataset))Dataset2 = ImageFolder(DIRECTORY2)print(len(Dataset))#%matplotlib inlineimg, label = Dataset[0]plt.imshow(img)img, label = Dataset[500]plt.imshow(img)img, label = Dataset2[21]plt.imshow(img)import torchvision.transforms as ttfrom torch.utils.data import DataLoaderfrom torchvision.utils import make_gridSizeOfImage = 256stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)#Create datasetsVangoghStyleDataset = ImageFolder(DIRECTORY, transform=tt.Compose([  tt.Resize(SizeOfImage),  tt.CenterCrop(SizeOfImage),  tt.ToTensor(),  tt.Normalize(*stats)])) dataset_photo = ImageFolder(DIRECTORY2, transform=tt.Compose([  tt.Resize(SizeOfImage),  tt.CenterCrop(SizeOfImage),  tt.ToTensor(),  tt.Normalize(*stats)]))DataloaderYForTest = DataLoader(VangoghStyleDataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)DataloaderXForTest = DataLoader(dataset_photo, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)dataloaderY = DataLoader(VangoghStyleDataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)dataloaderX = DataLoader(dataset_photo, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)#device = cuda.cuda_default()#trainDataLoader = DeviceDataLoader(trainDataLoader,device)#Check dataset & dataloader resultsiterDataY = iter(DataloaderYForTest)NextImgBatchY , clas = next(iterDataY)gridImage = make_grid(NextImgBatchY, nrow=4).permute(1, 2, 0).detach().numpy()gridY = reNormalize(gridImage)fig = plt.figure(figsize=(12, 8))plt.imshow(gridY)plt.axis('off')plt.title('monet')plt.show()#Check dataset & dataloader resultsiterDataX = iter(DataloaderXForTest)NextImgBatchX , clas = next(iterDataX)gridImage = make_grid(NextImgBatchX, nrow=4).permute(1, 2, 0).detach().numpy()gridX = reNormalize(gridImage)fig = plt.figure(figsize=(12, 8))plt.imshow(gridX)plt.axis('off')plt.title('photo')plt.show()from torch.nn import initdef weights_init_normal(m):#    Applies initial weights to certain layers in a model.#   The weights are taken from a normal distribution with mean = 0, std dev = 0.02.#    Param m: A module or layer in a network        #classname will be something like: `Conv`, `BatchNorm2d`, `Linear`, etc.    classname = m.__class__.__name__       #normal distribution with given paramters    std_dev = 0.02    mean = 0.0        # Initialize conv layer    if hasattr(m, 'weight') and (classname.find('Conv') != -1):        init.normal_(m.weight.data, mean, std_dev)def init_weights(m):    if isinstance(m, nn.Linear):        torch.nn.init.xavier_uniform(m.weight)        m.bias.data.fill_(0.01)def build_model(GenerativeConvDim=64, DiscriminativeConvDim=64, NumberOfResidualBlock=6):    #    Builds generators G_XtoY & G_YtoX and discriminators D_X & D_Y            #Generators    Generative_XtoY_Model = Generative_Discriminative_models.CycleGenerativeModel(convolution_dimention=GenerativeConvDim, NumberOfResidualBlock=NumberOfResidualBlock)    Generative_YtoX_Model = Generative_Discriminative_models.CycleGenerativeModel(convolution_dimention=GenerativeConvDim, NumberOfResidualBlock=NumberOfResidualBlock)        #Discriminators    Discriminative_X_Model = Generative_Discriminative_models.DiscriminativeModel(convolutional_dimention=DiscriminativeConvDim) # Y-->X    Discriminative_Y_Model = Generative_Discriminative_models.DiscriminativeModel(convolutional_dimention=DiscriminativeConvDim) # X-->Y        #Weight initialization    Generative_XtoY_Model.apply(init_weights)    Generative_YtoX_Model.apply(init_weights)    Discriminative_X_Model.apply(init_weights)    Discriminative_Y_Model.apply(init_weights)    #Moves models to GPU, if available    if torch.cuda.is_available():        device = torch.device("cuda:0")        Generative_XtoY_Model.to(device)        Generative_YtoX_Model.to(device)        Discriminative_X_Model.to(device)        Discriminative_Y_Model.to(device)        print('Models will work on GPU. ')    else:        print('Model will work on CPU. Only CPU available.')    return Generative_XtoY_Model, Generative_YtoX_Model, Discriminative_X_Model, Discriminative_Y_Model#Function callGenerative_XtoY_Model, Generative_YtoX_Model, Discriminative_X_Model, Discriminative_Y_Model = build_model()#Check model structuredef print_build(Generative_XtoY_Model, Generative_YtoX_Model, Discriminative_X_Model, Discriminative_Y_Model):    print("                     Generative_XtoY_Model                    ")    print("--------------------------------------------------------------")    print(Generative_XtoY_Model)    print(".___________________.")    print("                     Generative_YtoX_Model                    ")    print("--------------------------------------------------------------")    print(Generative_YtoX_Model)    print(".___________________.")    print("                      Discriminative_X_Model                      ")    print("--------------------------------------------------------------")    print(Discriminative_X_Model)    print(".___________________.")    print("                      Discriminative_Y_Model                      ")    print("--------------------------------------------------------------")    print(Discriminative_Y_Model)    print(".___________________.")    print_build(Generative_XtoY_Model, Generative_YtoX_Model, Discriminative_X_Model, Discriminative_Y_Model)def MeanSquareErrorRealLoss(DiscriminatorOut, adverserialWeight=1):    LossMeanSquareError = torch.mean((DiscriminatorOut-1)**2)*adverserialWeight    return LossMeanSquareErrordef MeanSquareErrorFakeLoss(DiscriminatorOut, adverserialWeight=1):    LossMeanSquareError = torch.mean(DiscriminatorOut**2)*adverserialWeight    return LossMeanSquareErrordef CycleConsistentyLoss(realImage, ImageReconstructed, lambdaWeight=1):    LossReconstructed = torch.mean(torch.abs(realImage - ImageReconstructed))    return lambdaWeight*LossReconstructed def LossIdentity(realImage, ImageGenerated, IdentityWeight=1):    LossIdentitity = torch.mean(torch.abs(realImage - ImageGenerated))    return IdentityWeight*LossIdentitityimport torch.optim as optimizer#hyperparameterlr=0.0002 generative_parameters = list(Generative_XtoY_Model.parameters()) + list(Generative_YtoX_Model.parameters())#Optimizers for Generative_Model and Discriminative_ModelgenerativeOpt = optimizer.Adam(generative_parameters, lr, betas=(0.5, 0.999))discriminatorXopt = optimizer.Adam(Discriminative_X_Model.parameters(), lr, betas=(0.5, 0.999))discriminatorYopt = optimizer.Adam(Discriminative_Y_Model.parameters(), lr, betas=(0.5, 0.999))def trainModel(dataloaderX, dataloaderY, DataloaderXForTest, DataloaderYForTest, NumOfEpoch=1000):        #LossList over_Time    lossList = []        #Weights for computing the Error Impact on backpropagation    adverserialWeight = 0.5    lambdaWeight = 10    IdentityWeight = 5        #Get some fixed data from domains X and Y for sampling. Images are held constant throughout training and allow us to inspect the model's performance.    IterXtest = iter(DataloaderXForTest)    IterYtest = iter(DataloaderYForTest)    NextImgBatchXtest,clasx = next(IterXtest)    NextImgBatchYtest,clasy = next(IterYtest)        # batches per epoch    XIter = iter(dataloaderX)    YIter = iter(dataloaderY)    batchesPerEpoch = len(XIter)    DiscriminativeTotalLossAvg = 0.0    GenerativeTotalLossAvg = 0.0    #Loop through epochs    for i in range(1, NumOfEpoch+1):                if i % batchesPerEpoch == 0:            XIter = iter(dataloaderX)            YIter = iter(dataloaderY)        #GetImageBatchX from ITERATION LIST by next method         NextImgBatchX,clasxx = next(XIter)        #GetImageBatchY from ITERATION LIST by next method         NextImgBatchY,clasyy = next(YIter)        #IF  there is available GPU , move batchofimage to GPU , otherwise keep going with CPU        device=cuda.cuda_default()        #device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")        NextImgBatchX = NextImgBatchX.to(device)        NextImgBatchY = NextImgBatchY.to(device)        #............TRAINING DISCRIMINATORS PART ..........        ##FIRST PART TRAIN DISCRIMINATOR_X ON REAL AND FAKE IMAGES##        #zero_grad() method is used to clear out the gradients of all parameters that the optimizer is tracking.        discriminatorXopt.zero_grad()                # 1. Compute the discriminator losses on real images        outputX = Discriminative_X_Model(NextImgBatchX)        DiscriminativeXRealLoss = MeanSquareErrorRealLoss(outputX, adverserialWeight)                # Train with fake images        # 2. Generate fake images that look like domain X based on real images in domain Y        fakeImageBatchX = Generative_YtoX_Model(NextImgBatchY)        # 3. Compute the fake loss for D_X        outputX = Discriminative_X_Model(fakeImageBatchX)        DiscriminativeXFakeLoss = MeanSquareErrorFakeLoss(outputX, adverserialWeight)                # 4. Compute the total loss and perform backpropagation        LossDiscriminativeX = DiscriminativeXRealLoss + DiscriminativeXFakeLoss        LossDiscriminativeX.backward()        discriminatorXopt.step()                ##SECOND PART TRAIN DISCRIMINATOR_Y ON REAL AND FAKE IMAGES##        #zero_grad() method is used to clear out the gradients of all parameters that the optimizer is tracking.                discriminatorYopt.zero_grad()                        outputY = Discriminative_Y_Model(NextImgBatchY)        DiscriminativeYRealLoss = MeanSquareErrorRealLoss(outputY, adverserialWeight)                fakeImageBatchY = Generative_XtoY_Model(NextImgBatchX)                outputY = Discriminative_Y_Model(fakeImageBatchY)        DiscriminativeYFakeLoss = MeanSquareErrorFakeLoss(outputY, adverserialWeight)         DiscriminativeYLoss = DiscriminativeYRealLoss + DiscriminativeYFakeLoss        DiscriminativeYLoss.backward()        discriminatorYopt.step()                DiscriminativeTotalLoss = DiscriminativeXRealLoss + DiscriminativeXFakeLoss + DiscriminativeYRealLoss + DiscriminativeYFakeLoss                #.......   TRAIN GENERATIVE MODELS.....        ##THIRD PART TRAIN GENERATIVE_Y_TO_X MODEL on Real and reconstructed images        # plus transformation loss on Real imageS of Van Gogh, fake imageS of Nature ##        #zero_grad() method is used to clear out the gradients of all parameters that the optimizer is tracking.                generativeOpt.zero_grad()                fakeImageBatchX = Generative_YtoX_Model(NextImgBatchY)        outputX = Discriminative_X_Model(fakeImageBatchX)        LossGenerativeYtoX = MeanSquareErrorRealLoss(outputX, adverserialWeight)        reconstructedY = Generative_XtoY_Model(fakeImageBatchX)                LossReconstructedY = CycleConsistentyLoss(NextImgBatchY, reconstructedY, lambdaWeight=lambdaWeight)                LossIdentityY = LossIdentity(NextImgBatchY, fakeImageBatchX, IdentityWeight=IdentityWeight)                ##FOURTH PART TRAIN GENERATIVE_X_TO_Y MODEL on Real and reconstructed images        # plus transformation loss on Real imageS of NATURE, fake imageS of VAN GOGH ##        fakeImageBatchY = Generative_XtoY_Model(NextImgBatchX)                outputY = Discriminative_Y_Model(fakeImageBatchY)         LossGenerativeXtoY = MeanSquareErrorRealLoss(outputY, adverserialWeight)        reconstructedX = Generative_YtoX_Model(fakeImageBatchY)                LossReconstructedX = CycleConsistentyLoss(NextImgBatchX, reconstructedX, lambdaWeight=lambdaWeight)                LossIdentityX = LossIdentity(NextImgBatchX, fakeImageBatchY, IdentityWeight=IdentityWeight)                 GenerativeTotalLoss = LossGenerativeYtoX + LossGenerativeXtoY + LossReconstructedY + LossReconstructedX + LossIdentityY + LossIdentityX        GenerativeTotalLoss.backward()        generativeOpt.step()                  DiscriminativeTotalLossAvg = DiscriminativeTotalLossAvg + DiscriminativeTotalLoss / batchesPerEpoch        GenerativeTotalLossAvg = GenerativeTotalLossAvg + GenerativeTotalLoss / batchesPerEpoch                newbatch = batchesPerEpoch        if i % newbatch == 0:                       lossList.append((DiscriminativeTotalLossAvg.item(), GenerativeTotalLossAvg.item()))            true_epoch_n = int(i/batchesPerEpoch)            true_epoch_total = int(NumOfEpoch/batchesPerEpoch)            print('Epoch [{:5d}/{:5d}] | d_total_loss_avg: {:6.4f} | g_total_loss: {:6.4f}'.format(                    true_epoch_n, true_epoch_total, DiscriminativeTotalLossAvg.item(), GenerativeTotalLossAvg.item()))                      show_every = (batchesPerEpoch*10)        if i % show_every == 0:                      Generative_YtoX_Model.eval()            Generative_XtoY_Model.eval()            show_test(NextImgBatchY, NextImgBatchX, Generative_YtoX_Model, Generative_XtoY_Model)                       Generative_YtoX_Model.train()            Generative_XtoY_Model.train()        if i % batchesPerEpoch == 0:            DiscriminativeTotalLossAvg = 0.0            GenerativeTotalLossAvg = 0.0        return lossListbatchesPerEpoch = len(dataloaderY)NumberEpoch = 300numberOfEpochs = NumberEpoch * batchesPerEpochlosses = trainModel(dataloaderX, dataloaderY, DataloaderXForTest, DataloaderYForTest, NumOfEpoch=numberOfEpochs)"""DiscriminativeModel = cuda.to_device(Generative_Discriminative_models.DiscriminativeModel, device)LATENT = 128torchrandomgenerate = torch.randn(SizeOfBatch, LATENT, 1, 1) # CREATE LATENT TensorGenerated_Image = Generative_Discriminative_models.generativeModel(torchrandomgenerate)print(Generated_Image.shape)cuda.show_images(Generated_Image)def train_discriminator(Real_Image, discriminator_gradient):# Clear DiscriminativeModel gradients  discriminator_gradient.zero_grad()# Pass real images through DiscriminativeModel  real_preds = DiscriminativeModel(Real_Image)  real_targets = torch.ones(Real_Image.size(0), 1, device=device)  real_loss = F.binary_cross_entropy(real_preds, real_targets)  real_score = torch.mean(real_preds).item()# Generate fake images  latent = torch.randn(SizeOfBatch, LATENT, 1, 1, device=device)  Generated_Image = Generative_Discriminative_models.generativeModel(latent)  # Pass fake images through DiscriminativeModel  fake_targets = torch.zeros(Generated_Image.size(0), 1, device=device)  fake_preds = DiscriminativeModel(Generated_Image)  fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)  fake_score = torch.mean(fake_preds).item()  # Update DiscriminativeModel weights  discriminator_loss_real_and_fake = real_loss + fake_loss  discriminator_loss_real_and_fake.backward()  discriminator_gradient.step()  return discriminator_loss_real_and_fake.item(), real_score, fake_scoredef train_generator(generator_gradient):# Clear generativeModel gradients  generator_gradient.zero_grad()# Generate fake images  Latentt = torch.randn(SizeOfBatch, LATENT, 1, 1, device=device)  Generated_Image = Generative_Discriminative_models.generativeModel(Latentt)                                                           # Try to fool the DiscriminativeModel  Predictions = DiscriminativeModel(Generated_Image)  Labels = torch.ones(SizeOfBatch, 1, device=device)  generator_loss_PredictionAndLabels = F.binary_cross_entropy(Predictions, Labels)  # Update generativeModel weights  generator_loss_PredictionAndLabels.backward()  generator_gradient.step()  return generator_loss_PredictionAndLabels.item()from torchvision.utils import save_imagecreate_directory = 'generated'os.makedirs(create_directory, exist_ok=True)def save_samples(i, tensor, x=True):  Generated_Image = Generative_Discriminative_models.generativeModel(tensor)  geenerated_image_name = 'generated-images-{0:0=4d}.png'.format(i)  save_image(cuda.d_normalization(Generated_Image), os.path.join(create_directory, geenerated_image_name), nrow=8)  print('Saving', geenerated_image_name)  if x:    figure, xsubplot = plt.subplots(figsize=(10, 10))    xsubplot.set_xticks([]); xsubplot.set_yticks([])    xsubplot.imshow(make_grid(cuda.d_normalization(Generated_Image.cpu().detach()), nrow=8).permute(1, 2, 0))    base_latent = torch.randn(64, LATENT, 1, 1, device=device)save_samples(0, base_latent)from tqdm.notebook import tqdmimport torch.nn.functional as Fdef fit(epochs, learning_rate, start_idx=1):  torch.cuda.empty_cache()  # Losses & scores  generativeLoss = []  discriminativeLoss = []  generative_REALscore = []  discriminative_score = []  # Create optimizers  optional_discriminative = torch.optim.Adam(DiscriminativeModel.parameters(), lr=learning_rate, betas=(0.5, 0.999))  optional_generative = torch.optim.Adam(Generative_Discriminative_models.generativeModel.parameters(), lr=learning_rate ,betas=(0.5, 0.999))  for epoch in range(epochs):    for real_images, _ in tqdm(trainDataLoader):       # Train DiscriminativeModel      loss_d, loss_g, real_score = train_discriminator(real_images, optional_discriminative)      # Train generativeModel      fake_score = train_generator(optional_generative)  # Record losses & scores      generativeLoss.append(loss_g)      discriminativeLoss.append(loss_d)      generative_REALscore.append(real_score)      discriminative_score.append(fake_score)    # Log losses & scores (last batch)    print("Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}".format(    epoch+1, epochs, loss_g, loss_d, real_score, fake_score))    # Save generated images    save_samples(epoch+start_idx, base_latent)  return generativeLoss, discriminativeLoss, generative_score, discriminative_score    learning_rate = 0.0003Nepochs =350if __name__ == "__main__":       history = fit(Nepochs, learning_rate)       generativeLoss, discriminativeLoss, generative_score, discriminative_score = history    # Save the model checkpoints    torch.save(Generative_Discriminative_models.generativeModel.state_dict(), 'Generative.pth')    torch.save(DiscriminativeModel.state_dict(), 'Discriminative.pth')"""                                                              